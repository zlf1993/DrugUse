{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script will load all the candidate tweets into memory, find the maximum sentence length; \n",
    "### then, load the training data into memory, find the corresponding word2vec embedding of each unique word, and save a look up table to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yanpe\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the candidate data and find the maximum and mean length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  1492215\n",
      "Max length:  84\n",
      "Mean length:  10\n"
     ]
    }
   ],
   "source": [
    "# Load the tweets\n",
    "TWEETS_PATH = 'dataset/quick_data.csv'\n",
    "TWEETS_LIST = []\n",
    "with open(TWEETS_PATH, newline='') as csvfile:\n",
    "    csvreader = list(csv.reader(csvfile, delimiter=','))\n",
    "    for row in csvreader[1:]:\n",
    "        TWEETS_LIST.append(row[2]) # the third row is the original text\n",
    "        #TWEETS_LIST.append(row[1]) # the second row is the modified text\n",
    "print('Number of tweets: ',len(TWEETS_LIST))\n",
    "\n",
    "# Find the max length\n",
    "current_max_length = 0\n",
    "sum_length = 0\n",
    "for tweet in TWEETS_LIST:\n",
    "    if current_max_length < len(tweet.split(' ')):\n",
    "        current_max_length = len(tweet.split(' '))\n",
    "    sum_length += len(tweet.split(' '))\n",
    "print('Max length: ',current_max_length)\n",
    "print('Mean length: ', int(sum_length/len(TWEETS_LIST)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in training dataset:  12660\n",
      "Number of unique words:  17025\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "\n",
    "#TRAIN_DATA_PATH = 'dataset/train_set.csv'\n",
    "TRAIN_SET_NAME = 'train_set_with_predicted'\n",
    "TRAIN_DATA_PATH = 'dataset/'+TRAIN_SET_NAME+'.csv'\n",
    "\n",
    "TWEETS_LIST = []\n",
    "with open(TRAIN_DATA_PATH, newline='') as csvfile:\n",
    "    csvreader = list(csv.reader(csvfile, delimiter=','))\n",
    "    for row in csvreader[1:]:\n",
    "        TWEETS_LIST.append(row[2]) # the third row is the original text\n",
    "        #TWEETS_LIST.append(row[1]) # the second row is the modified text\n",
    "print('Number of tweets in training dataset: ',len(TWEETS_LIST))\n",
    "\n",
    "# Get all the unique words in training data\n",
    "UNIQUE_WORDS = []\n",
    "counter = 0\n",
    "for tweet in TWEETS_LIST:\n",
    "    counter+=1\n",
    "    for word in tweet.split(' '):\n",
    "        if word in UNIQUE_WORDS:\n",
    "            continue\n",
    "        else:\n",
    "            UNIQUE_WORDS.append(word)\n",
    "print('Number of unique words: ',len(UNIQUE_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Twitter Word2Vec model (dimension: k=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_PATH = '/Users/matthew/Downloads/GoogleNews-vectors-negative300.bin' # GoogleNews word2vec model\n",
    "MODEL_PATH = 'dataset/word2vec_twitter_model.bin'\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(MODEL_PATH, binary=True, limit=3000000) # load 3,000,000 word maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV words:  1539\n",
      "OOV ratio (%):  9.039647577092511\n"
     ]
    }
   ],
   "source": [
    "LOOKUP_DICT = dict()\n",
    "OOV_WORDS = [] # Words out of vocabulary (OOV)\n",
    "\n",
    "for word in UNIQUE_WORDS:\n",
    "    try:\n",
    "        LOOKUP_DICT[word] = np.array(w2v_model[word]) # add to lookup table\n",
    "    except:\n",
    "        OOV_WORDS.append(word)\n",
    "\n",
    "print('OOV words: ',len(OOV_WORDS))\n",
    "print('OOV ratio (%): ',100*len(OOV_WORDS)/len(UNIQUE_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save lookup table to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/'+TRAIN_SET_NAME+'.lookup', 'wb') as f:\n",
    "    pickle.dump(LOOKUP_DICT, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test loading the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15486\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with (open('dataset/'+TRAIN_SET_NAME+'.lookup', 'rb')) as openfile:\n",
    "    try:\n",
    "        LOADED_TABLE = pickle.load(openfile)\n",
    "        print(len(LOADED_TABLE))\n",
    "        print(LOADED_TABLE['you'] - LOOKUP_DICT['you'])\n",
    "    except EOFError:\n",
    "        print('ERROR!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
